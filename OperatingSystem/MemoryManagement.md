# Memory Management
## Background
* Main memory and registers are the only storage CPU can access directly
* Collection of processes are waiting on disk to be `brought into memory and be executed`
* `Multiple programs are brought into memory` to improve resource utilization and response time to users
* A process may be `moved between disk and memory` during its execution(e.g. virtual memory)

### How to refer memory in a program: Address Binding

* Address Binding - Compile Time
	* Program is written as symbolic code
	* Compiler translates symbolic code into `absolute code`
	* If starting location changes -> recompile

* Address Binding - Load Time
	* Compiler translates symbolic code into `relocatable code`
	* Relocatable code: Machine language that can be run from any memory location
	* If starting location changes -> reload the code

* Address Binding - Execution Time(Runtime)
	* Compiler translates symbolic code into logical-address(i.e. virtual-address) code
	* Special hardware(i.e. MMU) is needed for this scheme
	* Most general-purpose OS use this method

* Memory-Management Unit(MMU)
	* Hardware device that maps virtual to physical address
	* The value in the `relocation register is added to every address` generated by a user process at the time it is sent to memory

* Logical v.s. Physical Address
	* Logical address(a.k.a virtual address) - generated by CPU
	* Physical address - seen by the memory module
	* Compile-time & load-time address binding(logical address = physical address)
	* Execution-time address binding(logical address != physical address)
	* The user program deals with logical addresses; it never sees the real physical addresses

### How to load a program into memory: static/dynamic loading and linking

#### Dynamic loading
* Does the entire program must be in the memory for it to execute? No, we can use dynamic-loading
	* A routine(function call) is loaded into memory when it is called
* `Better memory-space utilization`
	* unused routine is never loaded
	* Particularly useful when large amounts of code are infrequently used(e.g. error handling code)
* `No special support from OS` is required implemented through program(library, API calls)

#### Static Linking
* linking: function call the code from libraries
* Static linking: libraries are combined by the loader into the program in-memory image
	* Waste memory: duplicated code
	* Faster during execution time
	* Static linking + Dynamic loading still can not prevent duplicated code

#### Dynamic Linking
* Dynamic linking: Linking postponed `until execution time`
	* `Only one code copy` in memory and `shared by everyone`
	* A stub is included in the program in-memory image for each lib reference
	* Stub call -> check if the referred lib is in memory -> in not, load the lib -> execute the lib
	* e.g. DDL(Dynamic link library) on Windows

## `Swapping`
* How to move a program between mem. & disk: swap
* Swapping: A process can be swapped out of memory to `backing store`, and later brought back into memory for contiguous execution
	* Also used by `midterm scheduling`, different from context switch
* `Backing store` - a chunk of disk, separated from file system, to provide direct access to these memory images
* Why swap a process
	* Free up memory
	* Roll out, roll in: swap lower priority process with a higher one

* Swap back memory location
	* If binding is done at compile/load time: swap back memory address must be the same
	* If binding is done at execution time: swap back memory address can be different

* A process to be swapped == `must be idle`
	* Solutions for preventing from swapping the process that is waiting for I/O:
 		* Never swap a process with pending I/O
 		* Or I/O operations are done through OS buffers(i.e. a memory space not belongs to any user processes)

* Process Swapping to Backing Store
	* Major part of swap time is transfer time; `total transfer time is directly proportional to the amount of memory swapped`

## Contiguous Memory Allocation

### Memory Allocation
* Fixed-partition allocation
	* Each process loads into one partition of fixed-size
	* `Degree of muli-programming` is bounded by the number of partitions

* Variable-size partition
	* Hole: block of contiguous free memory
	* Holes of various size are scattered in memory

* Multiple Partition(Variable-Size) Method
	* When a process arrives, it is allocated a hole large enough to accommodate it
	* The OS maintains info. on each `in-use` and `free-hole`
	* A free hole can be merged with another hole to form a larger hole

* Dynamic Storage Allocation Problem
	* How to satisfy a request of size n from a list of free holes
	* `First-fit` - allocate the 1st hole that fits
	* `Best-fit` - allocate the smallest hole that fits: Must search through the whole list
	* `Worst-fit` - allocate the largest hole: Must also search through the whole list
	* First-fit and best-fit better than worst-fit in terms of speed and storage utilization

### Fragmentation
* External fragmentation
	* Total free memory space is big enough to satisfy a request, but is not contiguous
	* `Occur in variable-size allocation`
* Internal fragmentation
	* Memory that is internal to a partition but is not being used
	* `Occur in fixed-partition allocation`

* Solution: `compactiion`
	* Shuffle the memory contents to place all free memory together in one large block `at execution time`
	* Only if binding is done at execution time

# References
* [Operating System Course by Jerry Chou](https://www.youtube.com/playlist?list=PLS0SUwlYe8czigQPzgJTH2rJtwm0LXvDX)